{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "979c9035-d82e-4941-ba3a-6c7beb078f84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/emtpb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "from sys import exit\n",
    "#os.chdir(Path(os.path.realpath('__file__')).parents[1])\n",
    "#sys.path.append(Path(os.path.realpath('__file__')).parents[1])\n",
    "current_dir = os.path.abspath(os.path.dirname('__file__'))\n",
    "current_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "os.chdir(current_dir)\n",
    "sys.path.append(current_dir)\n",
    "print(current_dir)\n",
    "\n",
    "#os.chdir('/lustre/groups/cbm01/code/alexander.ohnmacht/emtpb')\n",
    "#sys.path.append('/lustre/groups/cbm01/code/alexander.ohnmacht/emtpb')\n",
    "os.chdir('/vol/emtpb/emtpb')\n",
    "sys.path.append('/vol/emtpb/emtpb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a470e696-7500-465f-82b2-ec67e4617b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import joblib\n",
    "import json\n",
    "import sys\n",
    "import math\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "\n",
    "from econml import sklearn_extensions\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "#run = \"run6\"\n",
    "score = [\"\",\"_gsva\",\"_tan_088\",\"_secrier_065\"]\n",
    "model_type = [\"eln\",\"grf\"]\n",
    "response_type = [\"\",\"_auc\"]\n",
    "cancer_type = range(28)\n",
    "drug = range(700)\n",
    "\n",
    "# create all possible combinations\n",
    "combinations = list(itertools.product(score, model_type, response_type, cancer_type, drug))\n",
    "\n",
    "# create a DataFrame for each combination\n",
    "dfs = []\n",
    "for comb in combinations:\n",
    "    df = pd.DataFrame(list(comb)).T\n",
    "    df.columns = [\"score\", \"model_type\", \"response_type\", \"cancer_type\",\"drug\"]\n",
    "    dfs.append(df)\n",
    "\n",
    "# concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# initialize a counter\n",
    "counter = 0\n",
    "# iterate over the rows of the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    # increment the counter by 1 every 28th row\n",
    "    if i % len(range(700*28)) == 0:\n",
    "        counter += 1\n",
    "    # add the counter value to a new column\n",
    "    df.loc[i, 'run'] = str(counter+6)\n",
    "\n",
    "# save DataFrame df\n",
    "# df.to_csv(\"metadata/paper/benchmark_paper_exp3.csv\", index=True)\n",
    "df = pd.read_csv(\"metadata/paper/benchmark_paper_exp3.csv\", index_col=0, keep_default_na=False)\n",
    "\n",
    "verbose = False # verbose=T does not work for CE\n",
    "example = True # \"1159-GDSC2\" # False\n",
    "cv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ce60310-4cdc-4bac-8ada-f0067275e7de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emtpb: Parallel run 0\n",
      "emtpb: Updated manual parallel run 274620\n",
      "score            _secrier_065\n",
      "model_type                grf\n",
      "response_type                \n",
      "cancer_type                 0\n",
      "drug                      220\n",
      "run                        21\n",
      "Name: 274620, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# use one parameter dict\n",
    "if len(sys.argv)!=2:\n",
    "    argument = ['-',1]\n",
    "else:\n",
    "    print('cmd entry:', sys.argv)\n",
    "    argument = sys.argv\n",
    "which_run = int(argument[1]) - 1\n",
    "print(\"emtpb: Parallel run \"+str(which_run))\n",
    "### costum\n",
    "which_run = 274621-1\n",
    "###\n",
    "\n",
    "# assign values for run\n",
    "print(\"emtpb: Updated manual parallel run \"+str(which_run))\n",
    "params = df.iloc[which_run]\n",
    "\n",
    "print(params)\n",
    "run = 'run'+str(params['run'])\n",
    "score = params['score']\n",
    "model_type = params['model_type']\n",
    "response_type = params['response_type']\n",
    "cancer_type = params['cancer_type']\n",
    "index_drug = params['drug'] # was not there\n",
    "\n",
    "#define cancer types based on marisa\n",
    "EMTscores = pd.read_csv(\"metadata/EMTscores.csv\")\n",
    "cancertypes = np.concatenate((['PANCAN'], EMTscores['TCGA Desc'].unique()))\n",
    "#select to be screened\n",
    "EMTscores = pd.read_csv(\"metadata/EMTscores\"+str(score)+\".csv\")\n",
    "cancertypes_here = np.concatenate((['PANCAN'], EMTscores['TCGA Desc'].unique()))\n",
    "\n",
    "#end run if it does not exist\n",
    "if cancertypes[cancer_type] not in cancertypes_here:\n",
    "    exit(0)\n",
    "\n",
    "preds_dir = \"metadata/\"+run+\"/predictions/\"+cancertypes[cancer_type]+\"/\"\n",
    "model_dir = \"metadata/\"+run+\"/models/\"+cancertypes[cancer_type]+\"/\"\n",
    "#preds_dir = \"metadata/\"+run+\"/predictions/\"+cancertypes[cancer_type]+\"/\"\n",
    "#model_dir = \"metadata/\"+run+\"/models/\"+cancertypes[cancer_type]+\"/\"\n",
    "os.makedirs(preds_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "mut = pd.read_csv(\"metadata/matrix_mut_\"+cancertypes[cancer_type]+\".csv\")\n",
    "#mut = pd.read_csv(\"metadata/matrix_mut_\"+cancertypes[cancer_type]+\".csv\")\n",
    "resp_full = pd.read_csv(\"metadata/matrix_resp\"+response_type+\".csv\")\n",
    "cols = resp_full.columns[resp_full.columns.str.contains('GDSC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "171c6c61-31b3-460e-b137-c535039cfd6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def t_test(x, vadj=7.25): # 7.25 for 5x5 cv\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = len(x)\n",
    "    stat = np.mean(x) / np.sqrt(np.var(x)/n*vadj)\n",
    "    return 2 * stats.t.sf(np.abs(stat), n-1)\n",
    "\n",
    "\n",
    "def run_model_cv(X, y, outer_seed, inner_seed, model_dir, preds_dir, names, which_index, outer_folds = 5, inner_folds = 5, repeats = 5, baseline = False, save_models = False, model_type = \"eln\", check_for_trained = True, min_samples = 25, save = True):\n",
    "    \n",
    "    # predictions save path and check if its exists already\n",
    "    preds_filename = os.path.join(preds_dir, f\"predictions_{model_type}_{which_index}_{baseline}.csv\")\n",
    "    if check_for_trained:\n",
    "        if Path(preds_filename).exists():\n",
    "            print(\"emtpb: iteration \"+str(index)+\" already exists... skipping...\")\n",
    "            return\n",
    "    \n",
    "    # remove interesting variable for baseline\n",
    "    if baseline:\n",
    "        X = np.delete(X, 0, axis=1)\n",
    "    \n",
    "    # initialize list of all preds\n",
    "    all_all_predictions = []\n",
    "    all_all_truth = []\n",
    "    all_all_folds = []\n",
    "    all_all_effects = []\n",
    "   \n",
    "    # Initialize names\n",
    "    names = pd.DataFrame(names)\n",
    "    names = pd.concat([names]*repeats, ignore_index = True)\n",
    "    \n",
    "    # check size of X,y and save placeholder\n",
    "    if mat.shape[0] < min_samples: # minimum 25 samples needed (5 test samples in outer cv and 4 in inner)\n",
    "        print(\"Amount of columns is only \"+str(mat.shape[0])+\"... skipping\")\n",
    "        names.to_csv(preds_filename, index=False)\n",
    "        return\n",
    "    \n",
    "    # initialize the outer cross-validation\n",
    "    outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=outer_seed)\n",
    "\n",
    "    # initialize the inner cross-validation\n",
    "    inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=inner_seed)\n",
    "\n",
    "    # repeat the outer cross-validation 5 times\n",
    "    for repeat in range(repeats):\n",
    "        \n",
    "        # initialize the list to store the predictions\n",
    "        all_predictions = []\n",
    "        all_truth = []\n",
    "        all_folds = []\n",
    "        all_effects = []\n",
    "\n",
    "        # set the seed for the outer cross-validation\n",
    "        outer_cv.random_state = outer_seed + repeat\n",
    "\n",
    "        # loop through the splits of the outer cross-validation\n",
    "        for i, (train_index, test_index) in enumerate(outer_cv.split(X)):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            # initialize the model\n",
    "            \n",
    "            # elastic net\n",
    "            if model_type == \"eln\":\n",
    "                #model = ElasticNetCV(l1_ratio=[0.01, .1, .5, .9, .95, 1],\n",
    "                #                     cv=inner_cv,\n",
    "                #                     random_state=inner_seed,\n",
    "                #                     tol=0.01)\n",
    "                model = sklearn_extensions.linear_model.WeightedLassoCV(random_state = inner_seed)\n",
    "                \n",
    "            # random forest\n",
    "            if model_type == \"rf\":\n",
    "                model = RandomForestRegressor(random_state=inner_seed)\n",
    "                \n",
    "                \n",
    "            # neural network\n",
    "            if model_type == \"mlp\":\n",
    "                model = MLPRegressor(max_iter=1000,\n",
    "                                     random_state=inner_seed)\n",
    "                # Define the hyperparameters to search over\n",
    "                \n",
    "                hidden_layer_sizes = [10, 50, 100, 150, 200]\n",
    "                n_layers = [2, 3 , 4]\n",
    "                layer_permutations = list(itertools.chain(*[itertools.permutations(hidden_layer_sizes, n) for n in n_layers]))\n",
    "                \n",
    "                param_grid = {\n",
    "                    'hidden_layer_sizes': layer_permutations\n",
    "                    #'activation': ['relu'] # 'identity', 'logistic', 'tanh', \n",
    "                    #'learning_rate_init': [0.001]\n",
    "                }\n",
    "                grid_search = RandomizedSearchCV(model, param_grid, cv=inner_folds, n_iter = 10, random_state = i+repeat*35)\n",
    "                grid_search.fit(X, y)\n",
    "                model = grid_search.best_estimator_\n",
    "                \n",
    "                # Get the best hyperparameters\n",
    "                print(grid_search.best_params_)\n",
    "                \n",
    "                \n",
    "            ############################    \n",
    "            if model_type in [\"eln\",\"rf\",\"mlp\"]:\n",
    "                \n",
    "                # fit the model on the training data\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # make predictions on the test set\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                # save the predictions\n",
    "                all_predictions.append(y_pred)\n",
    "                all_truth.append(y_test)\n",
    "                all_folds.append([i]*len(y_test))\n",
    "                all_effects.append([0]*len(y_test))\n",
    "                \n",
    "            # causal modelling\n",
    "            if model_type in [\"grf\"]:\n",
    "\n",
    "                #basemodel = ElasticNetCV(l1_ratio=[0.01, .1, .5, .9, .95, 1],\n",
    "                #                         cv=inner_cv,\n",
    "                #                         random_state=inner_seed,\n",
    "                #                         tol=0.01)\n",
    "                \n",
    "                # fit the model on the training data\n",
    "                np.random.seed(123)\n",
    "                model = CausalForestDML(discrete_treatment=False, random_state=123) #, model_y=basemodel, model_t=basemodel)\n",
    "\n",
    "                if cv:\n",
    "                    #tmp = np.array(y) # np.random.permutation(\n",
    "                    X_red = np.delete(X_train,(0), axis=1) #\n",
    "                    X_red_test = np.delete(X_test,(0), axis=1) #\n",
    "\n",
    "                    # fit model\n",
    "                    model.fit(Y = y_train, T = X_train[:,0], X = X_red) #model.fit(tmp, mat[:,0], X=matt)\n",
    "                    summary = model.effect_inference(X_red_test) #, model.const_marginal_ate_interval(np.delete(mat,(0), axis=1))\n",
    "                    #print(summary)\n",
    "                    #print(summary.pvalue())\n",
    "                    \n",
    "                    # make predictions on the test set\n",
    "                    #y_pred = model.predict(X_test)\n",
    "                    #pdb.set_trace()\n",
    "                    #global T_TRAIN\n",
    "                    #T_TRAIN = X_train[:,0]\n",
    "                    #global T_TEST\n",
    "                    #T_TEST = X_test[:,0]\n",
    "                    #global X_TRAIN\n",
    "                    #X_TRAIN = X_red\n",
    "                    #global X_TEST\n",
    "                    #X_TEST = X_red_test\n",
    "                    #global Y_TRAIN\n",
    "                    #Y_TRAIN = y_train\n",
    "                    #global Y_TEST\n",
    "                    #Y_TEST = y_test\n",
    "                    #global MODEL\n",
    "                    #MODEL = model\n",
    "                    #return \n",
    "                    \n",
    "                    #MODEL.models_y[0][-1].predict(X_TEST) + MODEL.effect(X_TEST) * (MODEL.models_t[0][-1].predict(X_TEST)+T_TEST)\n",
    "                    #y_pred = model.effect(X_red_test)\n",
    "                    y_pred = model.models_y[0][-1].predict(X_red_test) + model.effect(X_red_test) * (X_test[:,0] + model.models_t[0][-1].predict(X_red_test))\n",
    "                    #                                         HAS BEEN +                                HAS BEEN -\n",
    "                    # save the predictions\n",
    "                    all_predictions.append(y_pred)\n",
    "                    all_truth.append(y_test)\n",
    "                    all_folds.append([i]*len(y_test))\n",
    "                    all_effects.append([np.array2string(np.array([summary.pvalue(),model.effect(X_red_test),model.effect_interval(X_red_test)], dtype=object))]*len(y_test)) # transformback with: transformed_array = np.fromstring(my_array.replace('\\n', '').replace(' ', ',').replace(',,',',').replace('(','').replace(')','')[1:-1], sep=',')\n",
    "                    #all_folds.append([i]*len(y_test))\n",
    "                    \n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    # ignore train and test because we don't need it here\n",
    "                    #tmp = np.array(y) # np.random.permutation(\n",
    "                    X_red = np.delete(X,(0), axis=1) #\n",
    "\n",
    "                    # fit model\n",
    "                    model.fit(y, X[:,0], X=X_red) #model.fit(tmp, mat[:,0], X=matt)\n",
    "                    summary = model.effect_inference(X_red) #, model.const_marginal_ate_interval(np.delete(mat,(0), axis=1))\n",
    "                    #print(summary)\n",
    "                    #print(summary.pvalue())\n",
    "                    \n",
    "                    # make predictions on the test set\n",
    "                    #y_pred = model.predict(X_test)\n",
    "                    y_pred = model.effect(X_red)\n",
    "\n",
    "                    # save the predictions\n",
    "                    all_predictions.append(y_pred)\n",
    "                    all_truth.append(y)\n",
    "                    all_folds.append([i]*len(y_test))\n",
    "                    all_effects.append([np.array2string(np.array([summary.pvalue(),model.effect(X_red),model.effect_interval(X_red)], dtype=object))]*len(y)) # transformback with: transformed_array = np.fromstring(my_array.replace('\\n', '').replace(' ', ',').replace(',,',',').replace('(','').replace(')','')[1:-1], sep=',')\n",
    "                    #all_folds.append([i]*len(y_test))\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                if cv:\n",
    "                    pass\n",
    "                else:\n",
    "                    break # exit the loop after causal estimation without cv\n",
    "                \n",
    "            # save the predictions\n",
    "            #all_predictions.append(y_pred)\n",
    "            #all_truth.append(y_test)\n",
    "            #all_folds.append([i]*len(y_test))\n",
    "\n",
    "            # save the model\n",
    "            model_filename = os.path.join(model_dir, f\"model_{model_type}_{which_index}_{baseline}_{repeat}_{i}.joblib\")\n",
    "            if save_models:\n",
    "                joblib.dump(model, model_filename)\n",
    "\n",
    "        # concatenate all the predictions from cv\n",
    "        all_predictions = np.concatenate(all_predictions)\n",
    "        all_truth = np.concatenate(all_truth)\n",
    "        all_folds = np.concatenate(all_folds)\n",
    "        all_effects = np.concatenate(all_effects)\n",
    "        \n",
    "        # save the cv results and append to resampling\n",
    "        all_all_predictions.append(all_predictions)\n",
    "        all_all_truth.append(all_truth)\n",
    "        all_all_folds.append(all_folds)\n",
    "        all_all_effects.append(all_effects)\n",
    "    \n",
    "    # concatenate all the predictions\n",
    "    all_all_predictions = np.concatenate(all_all_predictions)\n",
    "    all_all_truth = np.concatenate(all_all_truth)\n",
    "    all_all_folds = np.concatenate(all_all_folds)\n",
    "    all_all_effects = np.concatenate(all_all_effects)\n",
    "    \n",
    "    names[\"preds\"] = all_all_predictions\n",
    "    names[\"truth\"] = all_all_truth#np.concatenate([y]*repeats)\n",
    "    names[\"repeat\"] = np.concatenate([[i]*len(y) for i in range(repeats)])\n",
    "    names[\"folds\"] = all_all_folds\n",
    "    names[\"repeatfold\"] = names[\"repeat\"].astype(str) + names[\"folds\"].astype(str)\n",
    "    names[\"effects\"] = all_all_effects\n",
    "    \n",
    "    # save predictions \n",
    "    if save:\n",
    "        names.to_csv(preds_filename, index=False)\n",
    "        \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3543c0d8-3677-427a-890a-ff452011bd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emtpb: modelling iteration 220...\n",
      "empb: shape before removing nan is (1234, 770)\n",
      "empb: shape after removing nan is (447, 770)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e-02, tolerance: 2.520e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e-02, tolerance: 2.520e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.990e-02, tolerance: 2.520e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e-02, tolerance: 2.795e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.685e-02, tolerance: 2.683e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.522e-02, tolerance: 1.372e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.610e-02, tolerance: 1.372e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.486e-02, tolerance: 1.372e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.591e-02, tolerance: 2.438e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.959e-02, tolerance: 3.088e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.070e-02, tolerance: 2.451e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.790e-02, tolerance: 2.722e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.987e-02, tolerance: 2.409e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.401e-02, tolerance: 2.399e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.285e-02, tolerance: 3.152e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.545e-02, tolerance: 3.152e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.995e-02, tolerance: 3.152e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.525e-02, tolerance: 3.152e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.558e-02, tolerance: 3.010e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.131e-02, tolerance: 2.963e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.373e-02, tolerance: 2.963e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.154e-02, tolerance: 1.457e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.392e-02, tolerance: 1.457e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.257e-02, tolerance: 1.457e-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31841695739916714\n"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "for index in range(len(cols)):\n",
    "    \n",
    "    ### costum\n",
    "    #index = 0\n",
    "    ###\n",
    "    if not not example:\n",
    "        index = index_drug #np.where(cols == \"1559-GDSC2\")[0][0]\n",
    "    \n",
    "    print(\"emtpb: modelling iteration \"+str(index)+\"...\")\n",
    "    #index = np.where(cols == \"1559-GDSC2\")[0][0]\n",
    "\n",
    "    # get one response column\n",
    "    resp = resp_full.copy()\n",
    "    resp = resp[[\"COSMIC ID\",\"TCGA Desc\",cols[index]]]\n",
    "    mat = pd.merge(EMTscores, resp, how='outer')\n",
    "    if cancertypes[0] != \"PANCAN\":\n",
    "        mat = mat[mat['TCGA Desc'] == cancertypes[cancer_type]]\n",
    "    mat = pd.merge(mat, mut, how='outer')\n",
    "    mat = pd.get_dummies(mat)\n",
    "    mat_df = mat\n",
    "    print(\"empb: shape before removing nan is \"+str(mat.shape))\n",
    "    mat.dropna(inplace=True)\n",
    "    print(\"empb: shape after removing nan is \"+str(mat.shape))\n",
    "    if mat.shape[0] == 0:\n",
    "        df = pd.DataFrame([[0]*mat.shape[1]],columns=mat.columns)\n",
    "        mat = mat.append(df, ignore_index=True)\n",
    "    names = mat['COSMIC ID']\n",
    "    mat.drop(columns=['COSMIC ID'], inplace=True)\n",
    "    response = mat.iloc[:,1] # mat.iloc[:,1] (drug response), # mat.iloc[:,0] (emt score)\n",
    "    mat = mat.drop(mat.columns[1], axis=1)\n",
    "    scale = StandardScaler()\n",
    "    mat = scale.fit_transform(mat)\n",
    "\n",
    "    for baseline in [True,False]: # [True, False]\n",
    "        if model_type == \"grf\" and baseline == True:\n",
    "            continue # skip baseline calcs for causal modelling\n",
    "        df = run_model_cv(\n",
    "            X = mat,\n",
    "            y = np.array(response),\n",
    "            outer_seed = 53+53, \n",
    "            inner_seed = 53,\n",
    "            model_dir = model_dir,\n",
    "            preds_dir = preds_dir,\n",
    "            names = names,\n",
    "            which_index = index,\n",
    "            repeats = 5, #(1 if model_type == 'grf' else 5), # <------- SET TO 5\n",
    "            baseline = baseline,\n",
    "            model_type = model_type,\n",
    "            save_models = False,\n",
    "            check_for_trained = False, # HAS BEEN TRUE\n",
    "            outer_folds = 5,\n",
    "            save = True\n",
    "        )\n",
    "        \n",
    "        if df is not None:\n",
    "            test = df #pd.read_csv(preds_dir+\"predictions_\"+str(model_type)+\"_\"+str(index)+\"_False.csv\") # df if save=False\n",
    "            per = np.array(test.groupby(\"repeatfold\")[['preds','truth']].corr().iloc[1::2,0])\n",
    "            per[np.isnan(per)] = 0\n",
    "            print(np.mean(per)) #\n",
    "\n",
    "    if verbose:\n",
    "        test = pd.read_csv(preds_dir+\"predictions_\"+str(model_type)+\"_\"+str(index)+\"_False.csv\") # df if save=False\n",
    "        perfalse = np.array(test.groupby(\"repeatfold\")[['preds','truth']].corr().iloc[1::2,0])\n",
    "        perfalse[np.isnan(perfalse)] = 0\n",
    "        print(perfalse)\n",
    "        print(np.mean(perfalse)) # \n",
    "        test = pd.read_csv(preds_dir+\"predictions_\"+str(model_type)+\"_\"+str(index)+\"_True.csv\") # df if save=False\n",
    "        pertrue = np.array(test.groupby(\"repeatfold\")[['preds','truth']].corr().iloc[1::2,0])\n",
    "        pertrue[np.isnan(pertrue)] = 0\n",
    "        print(pertrue)\n",
    "        print(np.mean(pertrue)) #\n",
    "        print(\"t-test p=\"+str(t_test(perfalse-pertrue)))\n",
    "        \n",
    "    if not not example:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43b280-6f49-459c-9c42-75ecb333ea62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"emt_pb: done modelling \"+cancertypes[cancer_type]+\" for run \"+str(which_run)+\" and drug index \"+str(index_drug)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b60d57-19f3-4373-9b0a-c849a9fa0023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check feature importances\n",
    "#m = joblib.load(\"metadata/run1/models/SKCM/model_513_False_0_0.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32dae857-7385-4511-9f31-e01b91ef0168",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emtpb: modelling iteration 220...\n",
      "empb: shape before removing nan is (1234, 770)\n",
      "empb: shape after removing nan is (447, 770)\n"
     ]
    }
   ],
   "source": [
    "### costum\n",
    "#index = 0\n",
    "###\n",
    "if not not example:\n",
    "    index = index_drug #np.where(cols == \"1559-GDSC2\")[0][0]\n",
    "\n",
    "print(\"emtpb: modelling iteration \"+str(index)+\"...\")\n",
    "#index = np.where(cols == \"1559-GDSC2\")[0][0]\n",
    "\n",
    "# get one response column\n",
    "resp = resp_full.copy()\n",
    "resp = resp[[\"COSMIC ID\",\"TCGA Desc\",cols[index]]]\n",
    "mat = pd.merge(EMTscores, resp, how='outer')\n",
    "if cancertypes[0] != \"PANCAN\":\n",
    "    mat = mat[mat['TCGA Desc'] == cancertypes[cancer_type]]\n",
    "mat = pd.merge(mat, mut, how='outer')\n",
    "mat = pd.get_dummies(mat)\n",
    "mat_df = mat\n",
    "print(\"empb: shape before removing nan is \"+str(mat.shape))\n",
    "mat.dropna(inplace=True)\n",
    "print(\"empb: shape after removing nan is \"+str(mat.shape))\n",
    "if mat.shape[0] == 0:\n",
    "    df = pd.DataFrame([[0]*mat.shape[1]],columns=mat.columns)\n",
    "    mat = mat.append(df, ignore_index=True)\n",
    "names = mat['COSMIC ID']\n",
    "mat.drop(columns=['COSMIC ID'], inplace=True)\n",
    "response = mat.iloc[:,1] # mat.iloc[:,1] (drug response), # mat.iloc[:,0] (emt score)\n",
    "mat = mat.drop(mat.columns[1], axis=1)\n",
    "scale = StandardScaler()\n",
    "mat = scale.fit_transform(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d18fb7a0-d547-4726-bb92-46f7f496ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat\n",
    "y = np.array(response)\n",
    "outer_seed = 53+53\n",
    "inner_seed = 53\n",
    "model_dir = model_dir\n",
    "preds_dir = preds_dir\n",
    "names = names_save\n",
    "which_index = index\n",
    "repeats = 1 #(1 if model_type == 'grf' else 5), # <------- SET TO 5\n",
    "baseline = baseline\n",
    "model_type = model_type\n",
    "save_models = False\n",
    "check_for_trained = False # HAS BEEN TRUE\n",
    "outer_folds = 5\n",
    "save = True\n",
    "min_samples = 25\n",
    "inner_folds = 5\n",
    "outer_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9a39fca-888f-4bfc-b603-7856510e23e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e-02, tolerance: 2.520e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.014e-02, tolerance: 2.520e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.990e-02, tolerance: 2.520e-02\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.321e-02, tolerance: 2.795e-02\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (447) does not match length of index (55875)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 216\u001b[0m\n\u001b[1;32m    213\u001b[0m all_all_folds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_all_folds)\n\u001b[1;32m    214\u001b[0m all_all_effects \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_all_effects)\n\u001b[0;32m--> 216\u001b[0m \u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m all_all_predictions\n\u001b[1;32m    217\u001b[0m names[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_all_truth\u001b[38;5;66;03m#np.concatenate([y]*repeats)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m names[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepeat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(repeats)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3980\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3979\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4174\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4166\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4172\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4173\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4174\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4177\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4178\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4179\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4180\u001b[0m     ):\n\u001b[1;32m   4181\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:4915\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4915\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/common.py:571\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (447) does not match length of index (55875)"
     ]
    }
   ],
   "source": [
    "# predictions save path and check if its exists already\n",
    "preds_filename = os.path.join(preds_dir, f\"predictions_{model_type}_{which_index}_{baseline}.csv\")\n",
    "if check_for_trained:\n",
    "    if Path(preds_filename).exists():\n",
    "        print(\"emtpb: iteration \"+str(index)+\" already exists... skipping...\")\n",
    "        #return\n",
    "\n",
    "# remove interesting variable for baseline\n",
    "if baseline:\n",
    "    X = np.delete(X, 0, axis=1)\n",
    "\n",
    "# initialize list of all preds\n",
    "all_all_predictions = []\n",
    "all_all_truth = []\n",
    "all_all_folds = []\n",
    "all_all_effects = []\n",
    "\n",
    "# Initialize names\n",
    "names = pd.DataFrame(names)\n",
    "names = pd.concat([names]*repeats, ignore_index = True)\n",
    "\n",
    "# check size of X,y and save placeholder\n",
    "if mat.shape[0] < min_samples: # minimum 25 samples needed (5 test samples in outer cv and 4 in inner)\n",
    "    print(\"Amount of columns is only \"+str(mat.shape[0])+\"... skipping\")\n",
    "    names.to_csv(preds_filename, index=False)\n",
    "    #return\n",
    "\n",
    "# initialize the outer cross-validation\n",
    "outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=outer_seed)\n",
    "\n",
    "# initialize the inner cross-validation\n",
    "inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=inner_seed)\n",
    "\n",
    "# repeat the outer cross-validation 5 times\n",
    "for repeat in range(repeats):\n",
    "\n",
    "    # initialize the list to store the predictions\n",
    "    all_predictions = []\n",
    "    all_truth = []\n",
    "    all_folds = []\n",
    "    all_effects = []\n",
    "\n",
    "    # set the seed for the outer cross-validation\n",
    "    outer_cv.random_state = outer_seed + repeat\n",
    "\n",
    "    # loop through the splits of the outer cross-validation\n",
    "    for i, (train_index, test_index) in enumerate(outer_cv.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # initialize the model\n",
    "\n",
    "        # elastic net\n",
    "        if model_type == \"eln\":\n",
    "            #model = ElasticNetCV(l1_ratio=[0.01, .1, .5, .9, .95, 1],\n",
    "            #                     cv=inner_cv,\n",
    "            #                     random_state=inner_seed,\n",
    "            #                     tol=0.01)\n",
    "            model = sklearn_extensions.linear_model.WeightedLassoCV(random_state = inner_seed)\n",
    "\n",
    "        # random forest\n",
    "        if model_type == \"rf\":\n",
    "            model = RandomForestRegressor(random_state=inner_seed)\n",
    "\n",
    "\n",
    "        # neural network\n",
    "        if model_type == \"mlp\":\n",
    "            model = MLPRegressor(max_iter=1000,\n",
    "                                 random_state=inner_seed)\n",
    "            # Define the hyperparameters to search over\n",
    "\n",
    "            hidden_layer_sizes = [10, 50, 100, 150, 200]\n",
    "            n_layers = [2, 3 , 4]\n",
    "            layer_permutations = list(itertools.chain(*[itertools.permutations(hidden_layer_sizes, n) for n in n_layers]))\n",
    "\n",
    "            param_grid = {\n",
    "                'hidden_layer_sizes': layer_permutations\n",
    "                #'activation': ['relu'] # 'identity', 'logistic', 'tanh', \n",
    "                #'learning_rate_init': [0.001]\n",
    "            }\n",
    "            grid_search = RandomizedSearchCV(model, param_grid, cv=inner_folds, n_iter = 10, random_state = i+repeat*35)\n",
    "            grid_search.fit(X, y)\n",
    "            model = grid_search.best_estimator_\n",
    "\n",
    "            # Get the best hyperparameters\n",
    "            print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "        ############################    \n",
    "        if model_type in [\"eln\",\"rf\",\"mlp\"]:\n",
    "\n",
    "            # fit the model on the training data\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # make predictions on the test set\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # save the predictions\n",
    "            all_predictions.append(y_pred)\n",
    "            all_truth.append(y_test)\n",
    "            all_folds.append([i]*len(y_test))\n",
    "            all_effects.append([0]*len(y_test))\n",
    "\n",
    "        # causal modelling\n",
    "        if model_type in [\"grf\"]:\n",
    "\n",
    "            #basemodel = ElasticNetCV(l1_ratio=[0.01, .1, .5, .9, .95, 1],\n",
    "            #                         cv=inner_cv,\n",
    "            #                         random_state=inner_seed,\n",
    "            #                         tol=0.01)\n",
    "\n",
    "            # fit the model on the training data\n",
    "            np.random.seed(123)\n",
    "            model = CausalForestDML(discrete_treatment=False, random_state=123) #, model_y=basemodel, model_t=basemodel)\n",
    "\n",
    "            if cv:\n",
    "                #tmp = np.array(y) # np.random.permutation(\n",
    "                X_red = np.delete(X_train,(0), axis=1) #\n",
    "                X_red_test = np.delete(X_test,(0), axis=1) #\n",
    "\n",
    "                # fit model\n",
    "                model.fit(Y = y_train, T = X_train[:,0], X = X_red) #model.fit(tmp, mat[:,0], X=matt)\n",
    "                summary = model.effect_inference(X_red_test) #, model.const_marginal_ate_interval(np.delete(mat,(0), axis=1))\n",
    "                #print(summary)\n",
    "                #print(summary.pvalue())\n",
    "\n",
    "                # make predictions on the test set\n",
    "                #y_pred = model.predict(X_test)\n",
    "                #pdb.set_trace()\n",
    "                #global T_TRAIN\n",
    "                #T_TRAIN = X_train[:,0]\n",
    "                #global T_TEST\n",
    "                #T_TEST = X_test[:,0]\n",
    "                #global X_TRAIN\n",
    "                #X_TRAIN = X_red\n",
    "                #global X_TEST\n",
    "                #X_TEST = X_red_test\n",
    "                #global Y_TRAIN\n",
    "                #Y_TRAIN = y_train\n",
    "                #global Y_TEST\n",
    "                #Y_TEST = y_test\n",
    "                #global MODEL\n",
    "                #MODEL = model\n",
    "                #return \n",
    "\n",
    "                #MODEL.models_y[0][-1].predict(X_TEST) + MODEL.effect(X_TEST) * (MODEL.models_t[0][-1].predict(X_TEST)+T_TEST)\n",
    "                #y_pred = model.effect(X_red_test)\n",
    "                y_pred = model.models_y[0][-1].predict(X_red_test) + model.effect(X_red_test) * (X_test[:,0] + model.models_t[0][-1].predict(X_red_test))\n",
    "                #                                         HAS BEEN +                                HAS BEEN -\n",
    "                # save the predictions\n",
    "                all_predictions.append(y_pred)\n",
    "                all_truth.append(y_test)\n",
    "                all_folds.append([i]*len(y_test))\n",
    "                all_effects.append([np.array2string(np.array([summary.pvalue(),model.effect(X_red_test),model.effect_interval(X_red_test)], dtype=object))]*len(y_test)) # transformback with: transformed_array = np.fromstring(my_array.replace('\\n', '').replace(' ', ',').replace(',,',',').replace('(','').replace(')','')[1:-1], sep=',')\n",
    "                #all_folds.append([i]*len(y_test))\n",
    "\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                # ignore train and test because we don't need it here\n",
    "                #tmp = np.array(y) # np.random.permutation(\n",
    "                X_red = np.delete(X,(0), axis=1) #\n",
    "\n",
    "                # fit model\n",
    "                model.fit(y, X[:,0], X=X_red) #model.fit(tmp, mat[:,0], X=matt)\n",
    "                summary = model.effect_inference(X_red) #, model.const_marginal_ate_interval(np.delete(mat,(0), axis=1))\n",
    "                #print(summary)\n",
    "                #print(summary.pvalue())\n",
    "\n",
    "                # make predictions on the test set\n",
    "                #y_pred = model.predict(X_test)\n",
    "                y_pred = model.effect(X_red)\n",
    "\n",
    "                # save the predictions\n",
    "                all_predictions.append(y_pred)\n",
    "                all_truth.append(y)\n",
    "                all_folds.append([i]*len(y_test))\n",
    "                all_effects.append([np.array2string(np.array([summary.pvalue(),model.effect(X_red),model.effect_interval(X_red)], dtype=object))]*len(y)) # transformback with: transformed_array = np.fromstring(my_array.replace('\\n', '').replace(' ', ',').replace(',,',',').replace('(','').replace(')','')[1:-1], sep=',')\n",
    "                #all_folds.append([i]*len(y_test))\n",
    "\n",
    "                break\n",
    "\n",
    "            if cv:\n",
    "                pass\n",
    "            else:\n",
    "                break # exit the loop after causal estimation without cv\n",
    "\n",
    "        # save the predictions\n",
    "        #all_predictions.append(y_pred)\n",
    "        #all_truth.append(y_test)\n",
    "        #all_folds.append([i]*len(y_test))\n",
    "\n",
    "        # save the model\n",
    "        model_filename = os.path.join(model_dir, f\"model_{model_type}_{which_index}_{baseline}_{repeat}_{i}.joblib\")\n",
    "        if save_models:\n",
    "            joblib.dump(model, model_filename)\n",
    "\n",
    "    # concatenate all the predictions from cv\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_truth = np.concatenate(all_truth)\n",
    "    all_folds = np.concatenate(all_folds)\n",
    "    all_effects = np.concatenate(all_effects)\n",
    "\n",
    "    # save the cv results and append to resampling\n",
    "    all_all_predictions.append(all_predictions)\n",
    "    all_all_truth.append(all_truth)\n",
    "    all_all_folds.append(all_folds)\n",
    "    all_all_effects.append(all_effects)\n",
    "\n",
    "# concatenate all the predictions\n",
    "all_all_predictions = np.concatenate(all_all_predictions)\n",
    "all_all_truth = np.concatenate(all_all_truth)\n",
    "all_all_folds = np.concatenate(all_all_folds)\n",
    "all_all_effects = np.concatenate(all_all_effects)\n",
    "\n",
    "names[\"preds\"] = all_all_predictions\n",
    "names[\"truth\"] = all_all_truth#np.concatenate([y]*repeats)\n",
    "names[\"repeat\"] = np.concatenate([[i]*len(y) for i in range(repeats)])\n",
    "names[\"folds\"] = all_all_folds\n",
    "names[\"repeatfold\"] = names[\"repeat\"].astype(str) + names[\"folds\"].astype(str)\n",
    "names[\"effects\"] = all_all_effects\n",
    "\n",
    "# save predictions \n",
    "if save:\n",
    "    names.to_csv(preds_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a94349-713f-4b60-bccb-3a5e06a7b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "            test = df #pd.read_csv(preds_dir+\"predictions_\"+str(model_type)+\"_\"+str(index)+\"_False.csv\") # df if save=False\n",
    "            per = np.array(test.groupby(\"repeatfold\")[['preds','truth']].corr().iloc[1::2,0])\n",
    "            per[np.isnan(per)] = 0\n",
    "            print(np.mean(per)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba3931-6a9a-49c6-83b5-d72f2fb65692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
